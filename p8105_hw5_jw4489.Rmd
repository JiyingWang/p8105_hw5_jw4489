---
title: "p8105_hw5_jw4489"
output: github_document
date: "2023-11-15"
---

```{r, message = FALSE}
library(tidyverse)
library(rvest)
library(dplyr)
library(purrr)
library(tidyr)
library(broom)
```

## Problem 1

##### Describe the raw data

```{r}
raw_df1 = read.csv("hw5data/homicide-data.csv")

view(raw_df1)
```

According to the raw data, we can see that it has `r nrow(raw_df1)` observations, and `r ncol(raw_df1)` variables in total.

```{r}
df1 = raw_df1 |> 
  janitor::clean_names() |> 
  mutate(city_state = paste(city, state, sep = ", "))

view(df1)
```

```{r}
df1_summarized = df1 |>
  group_by(city_state) |>
  summarize(total_homicides = n(),
             unsolved_homicides = sum(disposition %in% c("Closed without arrest", "Open/No arrest")))

df1_summarized
```

##### Estimate the proportion of homicides that are unsolve

```{r}
baltimore_prop_test = 
  prop.test(df1_summarized |>
            filter(city_state == "Baltimore, MD") |>
            pull(unsolved_homicides),
            df1_summarized |>
            filter(city_state == "Baltimore, MD") |>
            pull(total_homicides))

baltimore_tidy = baltimore_prop_test |>
  broom::tidy()

baltimore_tidy
```

```{r}
estimate = baltimore_tidy |>
  pull(estimate)
conf_low = baltimore_tidy |>
  pull(conf.low)
conf_high = baltimore_tidy |>
  pull(conf.high)
```

Hence the estimated proportion is `r estimate`, and the confidence interval is (`r conf_low`, `r conf_high`).

##### Run prop.test for each of the cities in the dataset

```{r}
all_city_test = df1_summarized |>
  janitor::clean_names() |>
  mutate(prop_test_all = purrr::map2(unsolved_homicides, total_homicides, \(x, y) prop.test(x = x, n = y)),
         prop_test_tidy = purrr::map(prop_test_all, broom::tidy)) |> 
  select(- prop_test_all) |>
  unnest(prop_test_tidy) |>
  janitor::clean_names() |>
  select(city_state, estimate, conf_low, conf_high) |>
  mutate(city_state = fct_reorder(city_state, estimate))

all_city_test
```

##### Create the plot

```{r}
test_plot = all_city_test |>
  ggplot(aes(x = city_state, y = estimate)) + 
  geom_errorbar(aes(ymin = conf_low, ymax = conf_high)) +
  geom_point() +
  labs(title = "Estimates and CIs for each city",
       x = "City, State",
       y = "Proportion of Unsolved Homicides") +
  theme(axis.text.x = element_text(size = 5, angle = 45, hjust = 1))

test_plot
```

## Problem 2

##### Create a tidy dataframe containing data from all participants

```{r}
file_names = list.files("./hw5data/hw5p2", pattern = "*.csv")
```

```{r}
df2 = data.frame(file_names = file_names) |>
  mutate(data = map(file_names, ~read.csv(paste0("./hw5data/hw5p2", "/", .x)))) |>
  unnest(data) 
```

```{r}
df2_tidy = df2 |>
  mutate(subject_id = str_extract(file_names, "\\d+"),
         arm = str_extract(file_names, "^[a-z]*")) |>
  select(- file_names) |>
  select(subject_id, arm, everything()) |>
  pivot_longer(cols = starts_with("week_"),
               names_to = "week",
               values_to = "observations") |>
  mutate(week = as.numeric(substr(week, 6, 7)))

df2_tidy 
```

##### Make the spaghetti plot

```{r}
df2_plot = df2_tidy |>
  ggplot(aes(x = week, y = observations, color = arm)) +
  geom_line() +
  facet_wrap(~ subject_id) +
  labs(title = "Observations on each subject over time",
       x = "Week",
       y = "Value of Observations")

df2_plot
```

According to the plots generated, we can see that, the experimental group shows an increasing pattern overall, but the controlled group doesn't have any trends or patterns. Besides, we can also conclude that for most of the participants, the experimental group tends to have higher values than the controlled group. 

## Problem 3

##### Conduct a simulation to explore power in a one-sample t-test

###### Set the event
```{r}
set.seed(1)
n = 30
sigma = 5
alpha = 0.05
mu_list = c(0, 1, 2, 3, 4, 5, 6)
```

###### Set the function

```{r}
test_result = function(mu, n = 30, sigma = 5, alpha = 0.05, repeats = 5000) {
  result_df = data.frame(true_mean = rep(mu, repeats),
                         estimated_mean = numeric(repeats),
                         p_value = numeric(repeats)
                         )
  for (i in 1 : 5000) {
    simulated_df <- rnorm(n, mean = mu, sd = sigma)
    tidy_result = broom::tidy(t.test(simulated_df))
  result_df$estimated_mean[i] = mean(simulated_df)
  result_df$p_value[i] = tidy_result$p.value
  }
  result_df$power = mean(result_df$p_value < alpha)
  return(result_df)
}
```

###### Repeat the above for μ = {0, 1, 2, 3, 4, 5, 6}

```{r}
all_results = data.frame()

for (mu in mu_list) {
  one_result = test_result(mu)
  all_results = rbind(one_result, all_results)
}
```

###### Make the first plot

```{r}
plot1 = all_results |>
  ggplot(aes(x = true_mean, y = power)) +
  geom_point() +
  geom_line() +
  labs(title = "Association between effect size and power",
       x_axis = "The true value of μ",
       y_axis = "The proportion of times the null was rejected ")
       
plot1
```

From the plot generated, we can conclude that larger the effect size is, the test will get a higher power.  

###### Make the second plot

```{r}

```

